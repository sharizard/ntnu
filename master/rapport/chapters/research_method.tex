% !TEX encoding = UTF-8 Unicode
% !TEX root = ../main.tex
% !TEX spellcheck = en-US

\chapter{Research Methodology}
\label{chap:researchmethod}


%Architectural TD is a problem for many software projects today. Even though ATD is recognized in many studies, there is lack of focus on ATD in embedded systems. In this thesis, we will investigate how architectural TD can be identified in embedded software projects, and how it can be managed. This chapter describes the relevant research methods used in software engineering, and the methods that has been used in this thesis to answer the research questions. 

The nature of this thesis makes it suitable as an empirical research. To answer the research questions that was stated in Section \ref{sec:chap1designquesitons}, Chapter \ref{chap:intro}, an empirical research needs to be carried out in order to collect some data. This chapter provides a brief introduction to research methods in software engineering, and describes the research conducted in the thesis. Section \ref{sec:researchmethodsinsoftwareengineering} describes the relevant research methods in software engineering. Section \ref{sec:choiceofmethod} describes the research method that was chosen for this study. Section \ref{sec:researchprocess} presents the research process we have followed throughout this thesis, which includes our research design.



%%%%%%%%% SECTION RESEARCH METHODS IN SE %%%%%%%%%%%%


\section{Research Methods in Software Engineering}
\label{sec:researchmethodsinsoftwareengineering}
 This section presents the relevant research methods applied in software engineering research. 

Research is believed to be the most effective way of coming to know what is happening in the world\cite{bassey2003case}. Empirical software engineering is a field of research based on empirical studies to derive knowledge from an actual experience rather than from theory or belief\cite{empirical-research-SE}. Empirical studies can be explanatory, descriptive, or exploratory\cite{Wohlin:2000:ESE:330775}. 

There are two types of research paradigms that have different approaches to empirical studies\cite{Wohlin:2000:ESE:330775}; \textit{the qualitative}, and \textit{the quantitative} paradigm. \textit{Qualitative} research is concerned with studying objects in their natural setting\cite{Wohlin:2000:ESE:330775}. It is based on non-numeric data found in sources as interview tapes, documents, or developers' model. \textit{Quantitative} research is concerned with quantifying a relationship or to compare two or more groups\cite{Wohlin:2000:ESE:330775}. It is based on collecting numerical data. 


To perform research in software, it might be to understand the different research strategies that are available in software engineering. Oates\cite{Oates:2006:RIS:1202299} presents six different research strategies; \textit{survey, design and creation, case study, experimentation, action research}, and \textit{ethnography}. 

\textit{{Survey}} focuses on collection data from a sample of individuals through their responses to questions. The primary means of gathering qualitative or quantitative data are interviews or questionnaires. The results are then analyzed using patterns to derive descriptive, exploratory, and explanatory conclusions. 

\textit{{Design and creation}} focuses on developing new IT products, or artifacts. It can be a computer-based system, new model, or a new method. 

\textit{{Case study}} focuses on monitoring one single 'thing'; an organization, a project, an information system, or a software developer. The goal is to obtain rich, and detailed data. 

\textit{{Experimentation}} are normally done in laboratory environment, which provides a high level of control. The goal is to investigate cause and effect relationships, testing hypotheses, and to prove or disprove the link between a factor and an observed outcome. 

\textit{{Action research}} focuses on solving a real-world problem while reflecting on the learning outcomes. 

\textit{{Ethnography}} is used to understand culture and ways of seeing of a particular group of people. The researcher spends time in the field by participating rather than observing.




%%%%%%%%% SECTION OF OUR RESEARCH METHOD %%%%%%%%%%%%

\section{Choice of Research Method in this Thesis}
\label{sec:choiceofmethod}
The main purpose of this research project is to gain an understanding about the nature of TD in software design, and its potential sources in safety-critical system software in order to improve the management of software evolution. Based on the research questions stated in Chapter \ref{chap:intro}, we have applied the case study approach to our research as it fits our desire to study an industrial system. Case studies provide both quantitative and qualitative information about the system\cite{Oates:2006:RIS:1202299}, depending on the approach the case study is taking.


\subsection{Case Study Method}
\label{subsec:casestudymethod}
%  Introduce methods, why. Design/plan, justify the why part.  Why this approach was chosen
Case study is an empirical method to investigate a single phenomenon within a specific time space in real-life context\cite{Wohlin:2000:ESE:330775}. Case studies excels at bringing an understanding of why or how certain phenomena occur or to add strength to what is already known through previous research\cite{Wohlin:2000:ESE:330775,soysusan}. Runeson et al.\cite{Runeson:2009:GCR:1519313.1519324} suggests case study as the most appropriate research method to use when exploring how a problem behaves in a real life context. In addition, they conclude that case study is suitable for software engineering research. There has been suggested systematic approaches for organizing and conducting a research successfully\cite{soysusan,Runeson:2009:GCR:1519313.1519324}. According to Yin\cite{yin2003case}, a research design is an action plan from getting "here" to "there", where "here" is defined as the initial set of questions answered, and "there" is some set of conclusions about these questions. Moreover, a research design can be seen as a blueprint of research, dealing with at least four problems: \textit{what questions to study, what data are relevant, what data to collect}, and \textit{how to analyze the results}\cite{yin2003case}. Soy\cite{soysusan} proposes six steps that can be used when carrying out a case study:

\begin{enumerate}
	\item \textit{Determine and Define the Research Questions}: The first step involves establishing a research focus by forming questions about the problem to the studied. The researcher can refer to the research focus and questions over the course of study. 
	\item \textit{Select the Cases and Determine Data Gathering and Analysis Techniques}: The second step involves determining what approaches to use in selecting single or multiple real-life cases cases to examine, and which instruments and data gathering approaches to use. (whom we want to study, the case, cases, sample. and how we want to study it, design).
	\item \textit{Prepare to Collect Data}: The third step involves a systematic organization of the data to be analyzed. This is to prevent the researcher from being overwhelmed by the amount of data and to prevent the researcher from losing sight of the research focus and questions. 
	\item \textit{Collect Data in the Field}: This step involves collecting, categorizing, and storing multiple sources of data systematically so it can be referenced and sorted. This makes the data readily available for subsequent reinterpretation. 
	\item \textit{Evaluate and Analyze the Data}: The fifth step involves examining the raw data in order to find any connections between the research object and the outcomes with reference to the original research questions. 
	\item \textit{Prepare the Report}: In the final step, the researcher report the data by transforming the problem into one that can be understood. The goal of the written report is to allow the reader to understand, question, and examine the study.
\end{enumerate}

\section{Case Context}
\label{sec:casecontext}
% with whom /participants)One of the disadvantages using commercial is the constraints associated with obtaining permission to mine and publish findings.
We have chosen to study a commercial system by conducting a case study to study the consequences of DD in this thesis. The conducted case study took place at Autronica Fire and Security AS. Autronica is a leading innovator, manufacturer, and supplier of fire safety equipment and marine safety monitoring and surveillance equipment. Their headquarter is based in Trondheim, Norway. AutroSafe, a high-end distributed fire alarm system, is one of the products they offer. The product was first released around year 2000, and has been on sale since. The software of the product is mainly based of C/C++ source files. Project "Firmus" is the project name for the next generation AutroSafe. "Firmus" is a Latin word, which in English means: \textit{solid, firm, strong, steadfast, steady, stable, reliable, and powerful}. The goal with Project "Firmus" is to adopt newer technologies and technology standards that are used today. We had the opportunity to conduct our case study on Project "Firmus". This project is still in the development phase. The goal of this research is to identify DD before it potentially creates more maintenance effort in the future. As we mentioned earlier, the product is mainly based of C/C++ source code files. The software architecture of the Project "Firmus" is component-based, where the different source files are divided into different components. The system consists of 13 components, and 461 source code files in total. Test files are found inside each component. To ensure reuse of source code, the company have developed a library that is used by this system and other systems as well as they can be seen as a case itself. However, we have decided not to include library files in our analysis. Table \ref{tab:systemmetrics} summarizes the system metrics, which includes the test files.

% Summary of system: Written in C/C++ and has been in development since late 1990s. It is used widely on their products. We have studied a pilot project which goal is to use better design, technologies etc. This is a pilot project so it has not entered the evolution. Our goal is to identify TD that has accumulated recently, and we'd like to discover them before they get messy. 

\begin{table}[]
\centering
\caption{System Metrics for Project "Firmus"}
\label{tab:systemmetrics}
\begin{tabular}{|l|l|}
\multicolumn{2}{c}{\textbf{Project "Firmus"}} \\ \hline
Lines                      & 88465          \\ \hline
Lines of Code              & 49287          \\ \hline
Lines of Comments          & 23017          \\ \hline
Components                 & 13             \\ \hline
Files                      & 461            \\ \hline
Number of Classes          & 339           \\ \hline
\end{tabular}
\end{table}



% Få fram noe om at vi har kjørt analyse av kode med bruk og uten bruk av biblioteker. Skrive noe om detaljene bak analysen. Skrive om formler som er brukt for å regne ut ting, og hvor vi har gått for å regne ut statistikk. Hvordan har vi fått grafer, fra hvor har vi generert dem. Referer til dem. 

% 


\section{Research Process}
\label{sec:researchprocess}
A research process provides a systematic approach on how to fulfill the goal of a research. We have chosen to follow the principles of the six steps defined by Soy\cite{soysusan} in this study: \textit{1) Determine and Define The Research Questions, 2) Select the Cases and Determine Data Gathering and Analysis Techniques, 3) Prepare to Collect Data, 4) Data Collection, 5) Evaluate and Analyze Data}, and \textit{6) Prepare the Report}.

%1 {Experiences and Motivation -> RQ} -> 2 {Case Study, Documents, Qualitative, Type: Descriptive and Exploratory} -> 3 {Prepare, organize and structure the data (ISO 9126 for requirements)} -> 4 {Data collection} -> 5 {Data analysis} -> 6 {Findings, conclusion, report,  All steps and methods that has been conducted during the the research will be reported through this thesis. }

\subsection{Determine and Define the Research Questions} % Pre study, state of the art
\label{subsec:rq}
The first step of this study is to define the research goal, and the research questions. In our previous research\cite{forprosjekt}, we stated that we are interested in getting a deeper insight into the field of TD by studying an industrial system. There are many subcategories of TD (See Table \ref{tab:subcategories}. With regards to that, we have chosen to investigate DD in safety-critical systems.


An analysis of the state-of-the-art was carried out to determine what prior studies have determine about the topic of DD. The goal with the analysis is to determine and define the research questions. \textit{Google Scholar, ACM Digital Library, Scopus}, and \textit{IEEE Xplore Digitar Library} were tremendously used during the analysis to find research papers that are relevant for our thesis. The literature review showed that there have been very little prior research on DD in safety-critical systems. Research questions were defined based on the results from our literature review. We have defined four research questions, \textbf{RQ1-4}, which we have summarized in Table \ref{researchQuestionsChapter3}. These research questions will be our primarily driving force though this research.

\begin{table}[]
	\centering
	\caption{Research Questions}
	\label{researchQuestionsChapter3}
	\begin{tabular}{|l|p{8cm}|}
		\hline
		\textbf{RQ1} & How can DD be identified?     \\ \hline
		%\textbf{RQ2} & Why does DD accumulate? \\ \hline
		\textbf{RQ2} & What are the effects of DD?  \\ \hline
		\textbf{RQ3} & What kind of DD can be found in safety-critical systems? \\ \hline
		\textbf{RQ4} & How to pay DD? \\ \hline
	\end{tabular}
\end{table}



\subsection{Select the Cases and Determine Data Gathering and Analysis Techniques} % Case Context
%This study addressed the research questions. Furthermore, we gathered data by analyzing documents and code. %Additionally, interviews were used to get a deeper insight about the problems from the developers.
A representative context has be chosen to analyze and investigate the research questions. We have chosen to conduct an exploratory and descriptive case study in real-life context to obtain knowledge about the problem to be studied. Several e-mails were sent out to organizations within the embedded industry. After a few weeks, we managed to get in touch with Autronica Fire and Security. The case study took place at their main office in Trondheim for approximately six weeks. A brief description of Autronica can be found in Section \ref{sec:casecontext}. We were provided with a workspace and multiple data sources, including access to the software's source code, issue for the project, system requirements, and documentation for system design and code. Data were mainly extracted from these sources. 

The first step is to find the structural code and design attributes of the software system. OO-metrics can be used to assess the quality of the software. Therefore, design attributes can be identified by measuring the OO-metrics. Furthermore, as we mentioned in Section \ref{sec:designdebt}, code smell detection can be used to identify design violations in the source code. A part of the literature review was to get familiar with existing tools that has been used to address similar problems. There is a wide set of tools which enables software metrics measurement and automatic static analysis of code. Some of the tools are open-source, while other contain strict licensing. However, a few of the commercial tools provide license for academical purposes. We will only focus on the tools providing measurement for C/C++. We have listed the tools used in this thesis below.

\textit{Doxygen}\cite{doxygen} is a free software for generating documentation from annotated C++ sources. \textit{Doxygen} has the ability to generate documentation in HTML or in Latex. Since the documentation is extracted from the source code, it is easier to keep the documentation up to date. In addition, \textit{Doxygen} can be configured to extract the code structure from undocumented sources files, which makes it possible to visualize the relations between various elements in the software. \textit{Doxygen} is used by the company to keep the documentation up-to-date, and was therefore used in this project to learn about the system and to visualize the system. 

\textit{ArgoUML}\cite{argouml} is an open source UML modeling tool that includes support for all standard UML 1.4 diagrams. It features reverse engineering of C++ projects by reading C++ source files and generate and UML model and diagrams. \textit{ArgoUML} was primarily used to reverse-engineer the source code to extract class diagrams.

\textit{Enterprise Architect}\cite{enterprisearchi} is an UML modeling tool. It has the ability to produce UML diagrams from code. This tool was used to create class diagrams for each component, allowing us to identify possible code smells, such as \textit{Large Class} and \textit{Long Method} code smell. \textit{Enterprise Architect} is a commercial software, but they offer a 30-day trial version of the software. The reason we used this was because ArgoUML was not fully able to reverse-engineer the source code.

\textit{SonarQube}\cite{sonarsource2013sonarqube} is an open source platform for quality management of software code. It has the ability to monitor different types of TD. It supports multiple languages through plugins, including Java, C/C++, JavaScript, and PHP. A downside with \textit{SonarQube} is that some of the plugins requires a commercial license, and that some features included are not applicable for C++. This tool was used to detect code smells in Project "Firmus".

\textit{Understand}\cite{understnad} is a code static analysis tool. It supports dozen of languages, including Java, C/C++, Fortran and Python. Understand can help developers analyze, measure, visualize, and maintain source code. It includes many features, including dependency graph visualization of code, and various metrics about the code (e.g., CBO).

\textit{CppDepend}\cite{cppdepend} is a commercial tool for analyzing source code in C/C++. 

\textit{CppCheck}\cite{cppcheck} is an open-source static analysis tool for C/C++ source code. 

\textit{CppClean}\cite{cppclean} is another open-source static analysis tool for C/C++ source code. Among many functionalities, this tool supports finding unnecessary \textit{'\#includes'} in header files. 

\textit{CCCC}\cite{cccc} is able to measure source code related metrics. It is able to measure some of the metrics defined by Chidamber and Kemerer\cite{chidamber1994metrics}. We have used this tool to measure software metrics, and the results has been compared to the results from \textit{Understand}.



\subsection{Prepare To Collect Data}
Preparing for data collection is the third step of thiscase study. \textit{Doxygen} is used by the company to generate and keep system documentation up-to-date. We spent some time analyzing the system documentation to get familiar with the system. Moreover, \textit{Doxygen} has the ability to generate various diagrams, including inheritance diagrams, and dependency graphs. However, a downside with \textit{Doxygen} is that it does not allow us to interact with the diagrams. \textit{Doxygen} allows us to specify depth of the graphs that are being generated, but output can be very large, hence we had some troubles understanding the graphs. Furthermore, \textit{Doxygen} can generate a dependency graph for each file in a component, but it does does not provide full dependency graphs for a component. There are many tools that offers reverse engineering of C/C++ source code, so we decided to try out a few of them, including \textit{ArgoUML}, \textit{Enterprise Architect}, and \textit{Understand}. \textit{Understand}, \textit{CppClean}, \textit{CppDepend}, and \textit{CppCheck} will be used to extract design problems at code level by analyzing the source code. 

A Word document and an Excel spreadsheet were created to keep track of the extracted data so we can review it later for analysis. 


\subsubsection{Metrics Selection}
Various software metrics exist for system measurement. Unfortunately, there are no well known standardized set of software metrics aimed to measure safety-critical systems. Choosing right metrics for measuring safety-critical software is preferable because this kind of software is responsible for many things, such as human lives. To enchance quality in software systems, OO-metrics were established. They measure characteristics of OO-systems in a way to improve them. Many aspects have been defined to improve the quality of code using these metrics. We have mainly used the metric suite defined by Chidamber and Kemerer\cite{chidamber1994metrics}. This suite of metrics is widely cited and has been used by many researchers, and their metrics suite is the deepest research in OO-metrics investigation . For example, Rosenberg et al.\cite{rosenberg1999risk} have applied this set of metric in evaluation of many NASA projects. The projects were written in both C++ and Java. In addition to Chidamber and Kemerer's metrics, we have measured two additional metrics, one counting the number of instance methods, and the other counting the number of instance variables. The following metrics have been measured in this study by using \textit{Understand}:

% Using Understand, we measured the software metrics by analyzing the source code. In total, we extracted 9 metrics for each class on the system, excluding the tests.

\textbf{LCOM}: A method is cohesive when it performs a single task. Low cohesion increases complexity, and will increase the likelihood for errors during development process. In general, the desirable value for LCOM is to be lower. The average percentage of class methods using a given class instance variable

\textbf{DIT}: Measures the longest path from a given class to the root class in the inheritance hierarchy.

\textbf{CBO}: Measures number of other classes that are coupled to a particular class. The desirable value is lower. 

\textbf{NOC}: Measures number of subclasses from a given class.

\textbf{RFC}: It is the sum of number of methods that can be potentially executed in response to a message by an object of a class.

\textbf{NIM}: Measures number of instance methods in a class, which is methods that are accessible through an object of that class. 

\textbf{NIV}: Measures number of instance variables in a class, that is, variables that are only accessible through an object of that class. Instance variables are used to measure LCOM of a class. 

\textbf{WMC}: This metrics sums the \textit{CC} of a class by counting the \textit{CC} for each method.



% How is data collected, from where
\subsection{Data Collection}
The fourth step of the research process is to execute the plan that was created in step three. During the case study, data is collected from two different sources by using multiple tools to improve the reliability of the study. The first source of design flaws it to identify for code smells in the source code. Table \ref{tab:codesmell} in Section \ref{subsub:codesmell}, Chapter \ref{chap:sota}, summarizes the code smells that are presented by Fowler et al.\cite{fowler1999refactoring}. By using automatic static analysis tools, we were able to identify multiple code smells in the system. Most of the code smells were manually verified by the researcher by inspecting the class and dependency diagrams for the class in which code smell exists. For instance, \textit{Duplicated Code} code smell were identified using \textit{SonarQube}. We inspected each file with duplicated code to verify the results. Another example of a code smell we identified is the \textit{Long Method} code smell. \textit{Long Method} code smell was identified using \textit{CppDepend} and \textit{Understand}. The results were verified by reverse engineering the source code to generate UML class diagrams. At first, we used the built-in functionality in \textit{Doxygen} to generate the class diagrams. However, \textit{Doxygen} was not able to provide full class diagrams. Therefore, we had too look for other options. We came across ArgoUML, an open source alternative to generate UML diagrams by reverse engineering C/C++ code. After comparing some of the results with snippets from \textit{Doxygen} class diagrams, we noticed that \textit{ArgoUML} failed to reverse engineer parts of code that contains classes, and their corresponding relations to other classes. This led us to look for commercial software. Using \textit{Understand} and \textit{Enterprise Architect}, we were able to extract the class diagrams for the system by using their built-in reverse engineering functionality. \textit{Enterprise Architect} allow us to interact with the class diagram. In addition, it was able to create a class diagram for each component, which made the navigation much easier compared to \textit{Understand}. UML class diagrams were used to verify \textit{Large Class} code smell and \textit{Long Method} code smell.

The second source of design flaw identification is to measure the OO-metrics for the system. OO-metrics are used to manage, predict, and improve the quality of a software product\cite{rodriguez2001overview} by finding classes with large metric values. \textit{Understand C++} was used for measuring all the metrics values of the system. The reason we chose to use this tool is because it provides wide set of metrics to be measured. In addition, \textit{Scitools}, the developers behind \textit{Understand C++}, offer a 15 days trial for academical purposes. The metrics for C++ are divided into four groups: file, class, project, and method. We have mainly focused on extracting class metrics, which includes \textit{LCOM, DIT, CBO, NIV, NIM, NOC}, and \textit{WMC}. In our measurements, we decided to exclude the test classes as they may potentially affect the metrics in both positive and negative way. After analyzing our project, we had the opportunity to extract the measured in HTML format. However, we wanted to export the tables to \textit{Microsoft Excel 2016} to perform some calculations. We had to create a script to convert the HTML tables to Excel tables in order to compute descriptive statistics and graph creation. Descriptive statistics deal with presentation and numerical processing of a data set\cite{Wohlin:2000:ESE:330775}. After collecting experimental data, descriptive statistics may be used to describe and graphically present interesting aspects of the data set. The goal with descriptive statistics is to get a feeing for how the data set is distributed, and may be used to understand the nature of the data and identify abnormal or false data points. 

The HTML tables were imported to both Excel 2013 and Google Spreadsheet. After metrics extraction, descriptive statistics were computed for the whole project and for each component using formulas in \textit{Google Spreadsheet}. These statistics aims to give a measure of the value of the metrics for all the classes, which we can use to identify classes with weak metric values. In addition, graphs were created using \textit{Google Spreadsheet} and \textit{Microsoft Excel 2016}. 


The descriptive statistics that we have measured are:
\begin{itemize}
	\item \textbf{\textit{Minimum}}: The minimum value of a metric. \\ Google spreadsheet formula: \textit{MIN(value1; [value2; ...])}
	\item \textbf{\textit{Maximum}}: The maximum value of a metric. \\ Google spreadsheet formula: \textit{MAX(value1; [value2; ...])}
	\item \textbf{\textit{Sample Mean}}: The mean of the metric, that is, the average value of a metrics. It can be used to measure the center of the data. \\ Google spreadsheet formula: \textit{AVERAGE(value1; [value2; ...])}
	\item \textbf{\textit{Mean}}: Represents the middle value of a data set. It is calculated by sorting the samples in ascending order and picking the middle sample\cite{Wohlin:2000:ESE:330775}. \\ Google spreadsheet formula: \textit{MEDIAN(value1; [value2; ...])}
	\item \textbf{\textit{Standard Deviation}}: A measure of how spread out the numbers are. Higher values indicates greater spread. \\ Google spreadsheet formula: \textit{STDEV(value1; [value2; ...])}
	\item \textbf{\textit{Kurtosis}}: A measure of whether the data set have a peak or not. A positive kurtosis value indicates peaked data distribution, while negative value indicate flat data distribution. \\ Google spreadsheet formula: \textit{KURT(value1; [value2; ...])}
	\item \textbf{\textit{Skewness}}: A measure which gives information about the symmetry of data distribution. Normal distribution has a skewness equal zero. Positive skewness value indicate longer right tail with sample mean placed on the right side of the peak value. Negative skewness value indicate longer left tail with sample mean placed on the left side of the peak value. \\ Google spreadsheet formula: \textit{SKEW(value1; [value2; ...])}
\end{itemize}

Code smell data were collected using automatic static analysis tools. \textit{CppDepend, CppClean, CppCheck, SonarQube}, and \textit{Understand} were mainly used to detect the code smells. In addition, we have manually verified some of the detected code smells in order to determine whether the hits are false negative or false positive. For example, to verify Long Method code smell, we manually inspected the class in which the method is located. 

\subsection{Evaluate and Analyze the Data}
In the fifth step of the case study, we examined the data collected that was in the fourth step. Although OO-metrics are well-known, there is still a discussion about identifying their threshold and usage\cite{tarcisio}. In the previous step, we collected data from multiple OO-metrics. The data can be used to measure the software quality from different points of views, search for potential problems in the source code, and identify possible candidates in which inspection may potentially be needed. However, the usage of OO-metrics alone do not tell us which classes have acceptable metric values, and which classes need an inspection. In addition to OO-metric measurement, we also need to determine when the metric values represent a positive characteristic, and when the values becomes some sort of warning sign for possible unwanted aspect of the software. The boundaries between two sets of values are known as thresholds\cite{ferreira2012identifying}. Our goal with the analysis part is to identify the classes with metric values above a defined threshold.  

Threshold values were identified using statistical information method\cite{lanza2007object}. For each metric, we used descriptive statistics, the sample mean and the standard deviation, to derive its threshold values. There are three different threshold values we identified. The first value corresponds with the sample mean and represents the most typical value in the data set. The second value is calculated as the sum of the sample mean and the standard deviation. It represents high values of the metric, but acceptable in some cases. The last value is the second value multiplied with 1.5\cite{lanza2007object}. It represents extreme values and should not be present in the data set.
% Descriptive statistics are used to summarize data in a meaningful way. By examining the data, we may be able to identify some patterns in the data. The descriptive statistics we have measured are the following; minimum, maximum, median, sample mean, and standard deviation. The minimum and maximum value helps us to identify the smallest and largest data value in the data set. The median value is the midpoint of the data set. It helps us identifying which half of the observations are above and below the midpoint. Sample mean is the average of the data, which is found by summing all of the observations divided by the number of observations. Both median and sample mean measure the central tendency. At last, the standard deviation measures how spread out the data are from the mean. Higher values indicates greater spread in the data. Furthermore, we calculate the frequency distribution for the different metrics. This was done to make the findings more accessible by visualizing the data, and to interpret the data afterwards. The results are then compared with some of the defined thresholds in the literature, and our findings from the literature review. 

\subsection{Prepare the Report}
Lastly, the methods conducted in this research will be reported through this thesis. This involves all the steps that we have gone through this research. The report also includes findings from the literature review, and how they are related to our findings from the case study. Lastly, the report conclusion summarizes our contributions, and points out suggestions for further research, so that other researchers may apply these techniques in some other context to determine whether findings are similar to our research or not.


\section{Summary of the Research Design}
The research questions described in Subsection \ref{subsec:rq} were answered using the case study methodology and a literature review. The literature review helped us getting familiar with the topic of area, and to identify the tools needed to mine necessary data. A case study was conducted in collaboration with Autronica Fire and Security AS, where we analyzed a safety-critical system to investigate DD.

\cleardoublepage























