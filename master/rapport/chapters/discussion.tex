% !TEX encoding = UTF-8 Unicode
% !TEX root = ..\main.tex
% !TEX spellcheck = en-US

\chapter{Discussion}
\label{chap:discussion}

This thesis has investigated DD in safety-critical systems. Section \ref{sec:dis-analysis} analyzes the OO-metrics from Chapter \ref{chap:results} by applying a set of threshold values to uncover possible insecure parts of the code. Section \ref{sec:disc:researcheva} evaluates the research by answering the research questions. Section \ref{sub:threats_to_validity} discusses the validity of the results.


\section{Analysis of Object-Oriented Metrics by Applying Threshold Values}
\label{sec:dis-analysis}
Software metrics measurement in OO-software is important in terms of quality management\cite{tarcisio,ferreira2012identifying}, as software metrics can be used as predictors of fault-prone classes in OO-systems\cite{basili1996validation}. A study by Basili et. al\cite{basili1996validation} assessed Chidamber and Kemerers\cite{chidamber1994metrics} suite of OO-metrics as predictors of fault-prone classes. Their results implied that OO-metrics appear to be useful to predict class fault-proneness during early phases in the software life-cycle. However, it is hard to assess the quality of a software with single OO-metrics. 

A metric is meaningless without its threshold values. According to Tarcisio\cite{tarcisio}, software metrics are not effectively used in software industry due to the fact that for the majority of metrics, thresholds are not defined. Threshold is defined as values used to set ranges of desirable and undesirable metric values for measured software\cite{ferreira2012identifying}. Knowing thresholds for metrics allow us to assess the quality of a software, and we may be able to identify where in a design errors are likely to occur. Lanza et al.\cite{lanza2007object} present two ways to identify major sources for threshold values; \textit{statistical information} (i.e., thresholds that are based on statistical measurements), and \textit{general accepted semantics} (i.e., thresholds that are based on information which is considered common).  

We identified the following thresholds for the individual metrics using their descriptive statistics from Table \ref{tab:oometrics-firmus}. The goal with applying threshold values to the metrics is to identify the classes with major design violations. Threshold values have been identified using statistical information\cite{lanza2007object}. For each metric, we have used the the sample mean and standard deviation from Table \ref{tab:oometrics-firmus}. There are three different threshold values: \textit{low/good}, \textit{high/typical}, and \textit{extreme/bad}. \textbf{\textit{Low/good}} value corresponds with the mean, and represents the most typical value in the data set\cite{cais2014identifying}. Another way to calculate \textit{low/good} is by subtracting the standard deviation from the sample mean\cite{lanza2007object}. However, the standard deviation in the data set may be larger than the sample mean, which leads to negative threshold values. This is why we have chosen to use the median value. \textbf{\textit{High/typical}} value is the sum of the sample mean and the standard deviation. Classes with \textit{high/typical} values are acceptable, but they should be inspected if possible. \textbf{\textit{Extreme/bad}} value is simply the second threshold value multiplied with 1.5\cite{lanza2007object}. It is considered as extreme, and should not be included in the data set. 

%Although we applied modern techniques to threshold identification, our study still should be viewed as evaluation of one software system and our results should not be taken as a dogma. Our techniques can be  applied to another kind of software for which it is hard to gather data, but it is necessary to bear in mind, that the thresholds represent only local data and for general usage, the broader comparison should e used. We present our data as a base for possible future comparison with other studies and we hope our results can bring more light into the thresholds of safety critical software metrics. As a future endeavor,  e would like to study differences between metrics thresholds measured in this study and data from a system with at least some degree of similarity. We would like to compare our results with measurement of  hresholds of open-source operational systems (which appear to be the most similar to our software) 

Table \ref{tab:thresholds} presents the metrics and their threshold values. The table includes threshold values that has been proposed by other researchers for the selected metrics. Despite the fact we were able to derive threshold values, our study should still be viewed as evaluation of one software system. Although our techniques can be applied to other kinds of safety-critical software, it is necessary to review that our threshold represent only local data. For general usage, it is recommended to compare our threshold with the the threshold defined by other researchers.

\begin{table}[ht!]
\resizebox{\textwidth}{!}{
\centering
\caption{Thresholds for OO-software metrics}
\label{tab:thresholds}
\begin{tabular}{|l|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{3cm}|}
\hline
\textbf{Metric} & \textbf{Observed Value} 	& \textbf{Low} 		& \textbf{High}  & \textbf{Extreme} 	& \textbf{Recommended Value} \\ \hline
LCOM   			& 100			         	& 42				& 75   			 & 112.5 (100+)						& 72.5\cite{tarcisio}           \\ \hline
DIT 			& 4             			& 1					& 2				 & 3+						& 4\cite{tarcisio}, 5\cite{rosenberg1999risk,metricoverview,metricsguide}              \\ \hline
CBO    			& 30  			          	& 6					& 11			 & 17+						& 5\cite{rosenberg1999risk,metricsguide}, 14\cite{sahraoui2000can,phpdepend}     \\ \hline
NOC    			& 20 			           	& 0					& 2				 & 4+						& 3\cite{tarcisio}, 5\cite{metricsguide}, 10\cite{metricoverview}               \\ \hline
RFC    			& 115           			& 15				& 34			 & 51+						& 50\cite{rosenberg1999risk,metricoverview}      \\ \hline
NIM    			& 48            			& 8					& 15			 & 23+						& N/A              \\ \hline
NIV    			& 18            			& 2					& 5				 & 7+						& N/A            \\ \hline
WMC    			& 325  			         	& 19				& 51			 & 76+						& 34\cite{tarcisio}, 40\cite{rosenberg1999risk}, 50\cite{phpdepend}      \\ \hline
\end{tabular}}
\end{table}


%For 101-1000 classes:

%CBO: Good: 0-1, regular: 2-20, bad: 20+
%NIV: 0-1, 1-8, 8+
%NIM: 0-25, 6-50, 50+
%DIT: 2
%LCOM: 0, 1-20, 20+


%ARTIKLER: 
\iffalse
WMC, DIT, RFC CBO, and NOC metrics appear to be useful to predict fault-prone classes. 

WMC: It was shown to be somewhat significant, and their results were stronger for new and modified classes. As excpected, the larger WMC is, the larger the probability of fault detection. Internal complexity does not have a strong impact if the class is reused verbatim or with very slight modifications. In that case, class interface properties will have the most significant impact.

DIT: Shown to be very significant. The larget DIT, the larger probability of defect detection.

RFC: Very significant. The larger RFC, the larger probability of defect detection, especially for UI classes and new/modified classes, same reason as for WMC for extensively modified classes.

NOC: Very significant except in the case of UI casses, but the trend seem to be a contrary to what they expected The larger the NOC value, the lower the probability of defect detection. This trend can be explained by that most classes do not have more than one child, and that verbatim reused classes are somwwhat associated with large NOC.

CBO: Very significant, and particular for UI classes.

LCOM: NIV kan si noe om LCOM. Si at det er 10 instanse variabler, det er ikke sikkert at alle blir delt på mellom metodene i klassen. LCOM is also significcant for predicting fault proneness.

These metrics can be used to build a predictive model of fault-prone classes.

Their result show these values:
WMC: Max 99, min 1, med 9.5, mean 13.4, stdev 14.9
DIT 9,0,0,1.32,1.99
RFC 105, 0, 19.5, 33.91, 33.37
NOC 13 0 0 0.23, 1.54
LCOM 426, 0, 0, 9.7, 63.77
CBO 30 0 5 6.8 7.56
\fi


% Hva betyr våre findings, hvor verdifulle de kan være, og hvorfor.





% Large DIT values -> many inherited methoc, class behavour cannot be predicted
\subsubsection{Depth in Inheritance Tree}
%DIT Viewpoints from Chidamber:
%%- The deeper a class is in the hierarchy, the greater number of methods it is likely to inherit, making it more complext to predict its behaviour. 
%- Deeper trees consistute greater design complexity, since more methods and classes are involved.
%- The deeper a class is in the hierachy, the greater the potential reuse of inherited methods.
% In general, as the tree gets deeper, it constitutes greater design complexity as more classes and methods are involved. 
\textit{DIT} indicates how deep a class is in the inheritance tree. Classes with higher value of DIT are associated with higher defects\cite{subramanyam2003empirical}. It is evident that a deep inheritance makes software maintenance more difficult\cite{daly1996evaluating}. Moreover, higher degree of \textit{DIT} indicate a trade-off between increased complexity and increase reuseability. A low median value indicates that at least 50\% of the classes tend to be close to the root in the inheritance hierarchy by following Chidamber and Kemerer's\cite{chidamber1994metrics} guide to interpreting \textit{DIT} metric using descriptive statistics. A low median value had a typical value of 1 and 3 in their study. However, if the majority of \textit{DIT} values are below 2, it may represent a poor exploitation of the advantages of OO-design and inheritance, because a \textit{DIT} value of 2 and 3 indicates higher degree of reuse. 

Figure \ref{fig:ditdistribution} in Chapter \ref{chap:results} show that approximately 39\% of the classes have a \textit{DIT} value of 0, while 26.7\% of the classes have a \textit{DIT} value of 1. These classes are considered to be close to the root in the inheritance tree, and there may be a probability of not exploiting the advantages of OO-methodologies. Classes with high values of \textit{DIT} have shown to be very significant in identifying fault-prone classes\cite{basili1996validation,el2001prediction}. We have derived the following threshold values for the \textit{DIT} metric: a \textit{low/good value} of 1, a \textit{high/typical} value of 2, and an \textit{extreme/bad} value of 3. We observe that at least 26.7\% of the classes satisfies good value by applying these thresholds. However, the \textit{extreme/bad} value of 3 does not comply with the median value of 3, which is considered as a good value\cite{chidamber1994metrics}. Therefore, we have chosen to apply the recommended max value of 5. As shown in Figure \ref{fig:ditdistribution}, there are no classes with a \textit{DIT} value of 5 or more. However, there are two classes with \textit{DIT} value of 4. These results do indicate that reuse opportunities through inheritance is limited and perhaps compromised in favor of comprehensibility of the overall architecture. On the other hand, low values of \textit{DIT} suggest that appropriate design preferences are being followed by the company\cite{subramanyam2003empirical}.

% High probability of misuse of intheritance principles, more testing time is required.
\subsubsection{Number Of Children}
%NOC Viewpoints:
%- Greater the number of children, grater the reuse, since inheritance is a form of reuse
%- Greater the number of children, greater the likelihood of improper abstraction of the parent class. If a class have a large number of children, it may be a case of misuse of subclassing.
%- The number of children gives an idea of the potential influece a class has on the design. If a class has a large number of children, it may require more testing of the methods in that class
\textit{NOC} measures the number of children a class has. The greater number of children a class has, the greater potential of reuse, since inheritance is a form of reuse. However, large values of \textit{NOC} indicate a misuse of subclassing\cite{basili1996validation}. Moreover, classes may require more testing. Our results reveal a \textit{NOC} median value is 0 (See Table \ref{tab:oometrics-firmus}). The distribution of \textit{NOC} metrics in Figure \ref{fig:nocdistribution} shows that approximately 86.5\% of all classes have no children. In addition, Figure \ref{fig:nocdistribution} show that a small number of classes have many immediate subclasses. Both Chidamber and Kemerer\cite{chidamber1994metrics} and Basili et al.\cite{basili1996validation} have observed the similar median values for \textit{NOC} metric in their respective studies. These values suggest that designer may not be fully exploiting the advantages of inheritance as a basis for designing classes. Lack of communication could be another reason between class designers, which may hinder developers to reuse.

The following threshold values for \textit{NOC} metric were derived from its descriptive statistics in Table \ref{tab:oometrics-firmus}: a \textit{low/good value} of 0, a \textit{high/typical} value of 2, and an \textit{extreme/bad} value 4. These values can be compared to threshold values identified by Filo et al.\cite{tarcisio}. The difference is that they classify \textit{extreme/bad} value as 4. According to Figure \ref{fig:nocdistribution}, 91.2\% of the classes are classified as \textit{low/good}, 5.2\% of the classes are classified as \textit{high/typical}, and 3.6\% of the classes are classified as \textit{extreme/bad}. Classes classified as \textit{extreme/bad} are an indication of classes that may be hard to understand and maintain, and are potential candidates for inspection. However, similar to \textit{DIT} metric values, \textit{extreme/bad} values indicate limitation in reuse opportunities in favor of comprehensibility of the overall architecture. This suggest that appropriate design preferences for the system are probably being followed. 


% Large LCOM values present cohesive and independent class.
\subsubsection{Lack of Cohesion in Methods}

%Viewpoints: 
%- Cohesiveness of methods within a class is desirable, since it promotes encapsulation
%- Lack of cohesion implies classes should probably be split into two or more subclasses
%- Any measure of disparateness of methods helps identify design flaws in classes
%- Low cohesion increases complexity, thereby increasing the likelihood of errors during the development process.

\textit{LCOM} is related to the counting of methods using common attributes in a class. As mentioned, smaller values of \textit{LCOM} represent cohesive and independent classes, which is desirable since it promotes encapsulation. Larger values of \textit{LCOM} increase the complexity of the class, hence increasing the likelihood of errors during the development process. Our derived threshold values for \textit{LCOM} metric reveal a \textit{low/good} value of 42, a \textit{high/typical} value of 75, and an \textit{extreme/bad value} of 112.5. However, as we see in Figure \ref{fig:lcomdistribution}, there are no classes with \textit{LCOM} value larger than 100. This result is influenced by the special shape of the data from \textit{LCOM} metric. The recommended threshold value for \textit{LCOM} is 72.5. This value is similar to our definition of \textit{high/typical}. We chose to apply the recommended threshold value for \textit{LCOM} metric. We identified 41 classes with larger \textit{LCOM} than 72.5, which we have illustrated in Figure \ref{fig:lcomdistribution}. Our data reveal that \textit{LCOM} values seem to increase with the size of classes. Most classes with a high value of \textit{LCOM} revealed to have large number of methods. These methods indicate higher disparateness in the functionality provided by the class. Chidamber and Kemererer\cite{chidamber1994metrics} and Basili et al.\cite{basili1996validation} state that classes with large values of \textit{LCOM} could be more error prone, and more difficult to test. A refactoring option would be to split the classes into two or more classes that are more well defined in terms of behavior. Moreover, \textit{LCOM} metric can be used by the developers to keep track of whether the cohesion principle is adhered.





% class is sensitive to changes
\subsubsection{Coupling Between Object classes}

%Viewpoints:
%- Excessive coupling between object classes is detrimental to modular design and prevents reuse. The more independent a class is, the easier it is to reuse it in another application.
%- In order to improve the modularity and promote encapsulation, inter-oject class couples should be kept to a minimum. The larger number of couples, the higher the sensitivity to changes in other parts of the design, and therefore maintenance is more difficult.
%- A measure of couping is useful to determine how complex the testing of varioous parts of a design are likely to be. The higer the inter-object class coupling, the more rigorous the testing needs to be.
\textit{CBO} refers to the number couplings between object classes. Higher values of \textit{CBO} indicate the extent of lack of reuse potential of a class, and that more effort may be required to maintain and test the class. Classes with higher values of \textit{CBO} are associated with higher defects\cite{subramanyam2003empirical}. According to Chidamber and Kemererer\cite{chidamber1994metrics}, a low median had a typical value of 0, while a high median had a value of 9. A median value of 0 suggests that at least half of the classes are self-contained and do not refer to other classes. Furthermore, Basili et al.\cite{basili1996validation} state that \textit{CBO} appears to be useful to predict class fault-proneness. 

Table \ref{tab:oometrics-firmus} reveal a \textit{CBO} median value of 5, indicating that at least 50\% of the classes refer to 5 or less object classes. The \textit{CBO} value is generally less for most classes. Therefore, these classes are easy to understand, reuse, and maintain. However, we did notice that some of the classes with low \textit{CBO} value had higher values in the other metrics, such as \textit{WMC} and \textit{LCOM}. 

Threshold values for the \textit{CBO} metric were derived, and the following values were identified: a \textit{low/good} value of 6, a \textit{high/typical} value of 11, and an \textit{extreme/bad} value of 17 or more. Chidamber and Kemerer\cite{chidamber1994metrics} classified a median value of 9 as high, which is similar to the recommended values in Table \ref{tab:thresholds}. Therefore, we have chosen to apply our derived \textit{high/typical} threshold value of 11 on the \textit{CBO} metric results. Figure \ref{fig:cbodistribution} reveal that 11.3\% of all classes have a coupling of 11 or more. \textit{CBO} should be kept to a minimum in order to promote encapsulation and modularity for a class. Classes with large values of \textit{CBO} are more sensitive to changes in other parts of the design, which affects the systems maintainability, reusability, and testability. Furthermore, the \textit{CBO} metric can be used as a way to track whether the class hierarchy is losing its integrity, and whether different parts of the system are developing unnecessary relations between classes.


% many methods can call the class, that affects its complexity
\subsubsection{Response For Class}
%Viewpoints:
%- If a large number of methods can be invoked in response to a message, the testing and debugging of the class becomes more complicated since it requires a greater level of understanding required on the part of the tester. 
%- The larger the number of methods that can be invoked from a class, the grater the complexity of the class.
%- A worst case of value for possible responses will assist in appropriate alocation of testing time.
\textit{RFC} is defined as the total number of methods that can be executed in response to a message to a class. This count includes all methods available in the class hierarchy. Large values of \textit{RFC} makes testing and debugging more complicated since it requires a greater level of understanding on the part of the tester\cite{chidamber1994metrics}. In addition, large values of \textit{RFC} may increase the complexity of the class. It can be hard to predict the behavior of the class since it requires a deep understanding of the potential interactions that the objects of the class have with the rest of the system, hence affecting the classes' testability. Our analysis reveal that most classes are able to invoke a small number of methods. According to Figure \ref{fig:rfcdistribution}, 74.2\% of the classes have a \textit{RFC} value of less than 15.

The following threshold values were derived for the \textit{RFC} metric: a \textit{low/good value} of 15, a \textit{high/typical} value of 34, and an \textit{extreme/bad} value of 51 or more. The \textit{extreme/bad} value is very similar to the recommended max value, although it is acceptable to have a \textit{RFC} up to 100\cite{metricoverview}. However, it is recommended that a class does not have \textit{RFC} value larger than 50. We applied the threshold value on Project \textit{"Firmus"}, and identified 17 classes with a \textit{RFC} value larger than 50. One of the class have its \textit{RFC} at 115. Research point out that \textit{RFC} has been found to be highly correlated with \textit{WMC} and \textit{CBO}\cite{chidamber1994metrics}. Our results reveal that RFC is not correlated with either \textit{WMC} or \textit{CBO}. For example, the class with \textit{RFC} value of 115 had a \textit{CBO} value of 2 and \textit{WMC} value of 22. However, its \textit{DIT} value is 4 and may explain that the large values of \textit{RFC} comes from inherited methods. We identified the same pattern with the class with \textit{RFC} of 109. This particular class has a \textit{CBO} value of 3 and a \textit{WMC} value of 10. However, its \textit{DIT} value is 4. This observation indicates that \textit{RFC} is associated with \textit{DIT}. Moreover, \textit{RFC} values tend to be low because there are a number of classes that have no parents.


%\subsubsection{Number Of Methods and Number of Instance Methods}
%Generelt ønsker man å ha flere små metoder i en klasse enn et par store. Dersom LCOM ikke stemmer kan klassen bli veldig stor og det kan også si ne om at klassen er en code smell som bør splittes opp.

%\subsubsection{Number of Instance Variables}

%"GATHERED OO METRICS ARE ANALYZED TO POINT OUT CANDICATE CLASSES. THESE CLASSES NEEDS CODE ISPECTION AND POTENTIALLY REFACTORED USING TOOLS. CLASSES THAT NEEDS REFACTORING PROCESS ARE THOE WITH HIGH VALUE FOR OO METRICS AND HIGH VALUES FOR CLASS SIZE AND OVERALL COMPLEXITY"

% Complex class
\subsubsection{Weighted Methods per Class}
\textit{WMC} refers to the sum of complexity in each method. A greater value of \textit{WMC} indicates a complex class. \textit{WMC} is usually affected by the number of methods in a class, but there may be some cases where some methods may have low complexity while other methods have high complexity. Table \ref{tab:oometrics-firmus} reports a median value of 10 and sample mean value of 19.707 on \textit{WMC} metric. The results are quite similar to what Chidamber and Kemerer\cite{chidamber1994metrics} and Basili et al.\cite{basili1996validation} measured in their respective studies. Another aspect of \textit{WMC} data is the similarity of the frequency distribution of the metric values in both of their and our study. In Figure \ref{fig:wmcdistribution}, we notice that approximately 60\% of all classes have a \textit{WMC} value of less than 10. This suggest that most classes have a small number of methods. The following threshold values were derived for \textit{WMC} metric: a \textit{low/good} value of 19, \textit{high/typical} value of 51, and an \textit{extreme/bad} value of 76. According to Table \ref{tab:thresholds}, the recommended max value are ranging between 30-50, which is close to our \textit{high/typical} value. Therefore, it seems like this value is more appropriate to use to identify classes with high values of \textit{WMC}. We apply the threshold, and identify 7.4\% (i.e., 17 classes) with a \textit{WMC} value larger than 51. One of the classes has a \textit{WMC} value of 325, which indicates that time and effort to develop and maintain this class may be high. The same class have 44 methods and a \textit{CBO} value of 10. A class with many methods are most likely to be application specific, hence reducing its reuse potential. Moreover, an increase in number of methods is associated with an increase in defects\cite{subramanyam2003empirical}. Another class with a \textit{WMC} value of 194 reveal to have 40 methods. This suggest that classes with more methods tend to have higher complexity, which indicates an increase in defects. In such cases, maintenance effort increase drastically. The 17 classes with large value of \textit{WMC} are primary candidate classes in which code inspection and potentially refactoring is needed.


%The classes above the recommended values can be seen as outliners. 



%Classes which are infected with God Class, and God Methods have higher class error probability than non-infected classes.




\section{Research Evaluation}
\label{sec:disc:researcheva}
%Table "X" contains a summary of the resuls of this research. Although, the research did not specify the effects of DD in embedded systems, we still were able to identify some of the effects it had on

%The goal with this case study conducted at Autronica in Trondheim is to find ways that can help us to identify DD in embedded systems. One approach of doing this is to measure the software quality using OO-metrics. Using OO-metrics to measure the system can help us to identify poorly designed classes, which also helps us to answer the first research question.

This thesis is focused on identifying DD in safety-critical systems. In particular, we were able to identify DD by measuring OO-metrics and detecting code smells. The software design was analyzed using a suite of OO-metrics proposed by Chidamber and Kemererer, one of the most used OO-measures. The result does indicate that embedded software developers accumulate TD, despite the fact that they cannot contain any errors\cite{pretschner2007software,ebert2009embedded,trienekens2010quality}. However, it seems like the accumulated TD is manageable. An important aspect when delivering a product is to make sure the product is stable and reliable. According to our results, we can now answer the research questions that were stated in Chapter \ref{chap:intro}. Table \ref{tab:conribution} show the connection between our contributions, and research questions.

\begin{table}[ht!]
\centering
\caption{Connection between contributions, and research questions}
\label{tab:conribution}
\begin{tabular}{|l|p{6cm}|l|}
\hline
\textbf{Contributions} & \textbf{Description}                                                                                                               & \textbf{Research Questions} \\ \hline
\textbf{C1}            & Empirical knowledge about DD identification in safety-critical systems by object-oriented metric analysis and code smell detection. & RQ1                         \\ \hline
\textbf{C1.1}          & A set of threshold values for the measured OO-metrics. See Table \ref{tab:thresholds}.                                                                 & RQ1                         \\ \hline
\textbf{C2}            & Empirical knowledge about the effects of having DD in safety-critical systems.                                                      & RQ2                         \\ \hline
\textbf{C3}            & Empirical knowledge about the different types of DD in safety-critical systems.                                                     & RQ3                         \\ \hline
\textbf{C4}                     & Empirical knowledge about paying DD.                                                                                                & RQ4                         \\ \hline
\end{tabular}
\end{table}


%WMC, DIT, RFC CBO, and NOC metrics appear to be useful to predict fault-prone classes. 

%WMC: It was shown to be somewhat significant, and their results were stronger for new and modified classes. As excpected, the larger WMC is, the larger the probability of fault detection. Internal complexity does not have a strong impact if the class is reused verbatim or with very slight modifications. In that case, class interface properties will have the most significant impact.

%DIT: Shown to be very significant. The larget DIT, the larger probability of defect detection.

%RFC: Very significant. The larger RFC, the larger probability of defect detection, especially for UI classes and new/modified classes, same reason as for WMC for extensively modified classes.

%NOC: Very significant except in the case of UI casses, but the trend seem to be a contrary to what they expected The larger the NOC value, the lower the probability of defect detection. This trend can be explained by that most classes do not have more than one child, and that verbatim reused classes are somwwhat associated with large NOC.

%CBO: Very significant, and particular for UI classes.

%LCOM: NIV kan si noe om LCOM. Si at det er 10 instanse variabler, det er ikke sikkert at alle blir delt på mellom metodene i klassen. LCOM is also significcant for predicting fault proneness.

%These metrics can be used to build a predictive model of fault-prone classes.




\subsubsection{RQ1: How can DD be identified?} 
The first research question is related to the techniques we have used to identify DD in this research. Automatic static code analysis, and OO-metrics measurements using Chidamber and Kemerer's suite of metrics have proven to be useful in the context of DD identification. We were able to collect a large amount of measurements that characterize the software by using various tools. Tools that have been used through this research are \textit{Doxygen}, \textit{Understand}, \textit{SonarQube}, \textit{CCCC}, \textit{CppCheck}, \textit{CppDepend}, and \textit{Enterprise Architect}.

Code smells are an example of design flaws that can degrade maintainability of source code, which implies that code smells can be used as an indicator to identify fault-prone files in the system. Moreover, code smells are an indication of refactoring possibilities in code base. We have been able to identify \textit{Duplicated Code}, \textit{Long Method}, \textit{Long Parameter List}, \textit{Speculative Generality}, \textit{Dead Code}, and \textit{Large Classes} code smells in our analysis. \textit{Duplicated Code} was identified using \textit{SonarQube}. \textit{SonarQube} analyzed the entire source code base, and identified similar code blocks that had an appearance in multiple places. \textit{Long Method}, \textit{Speculative Generality}, and \textit{Dead Code} were identified using \textit{CppDepend}, \textit{Understand}, and \textit{CCCC}. \textit{Long Parameter List} code smell were detected using \textit{CppDepend} and confirmed using \textit{Enterprise Architect}.

Another approach to assessing the software quality is based on OO-metrics\cite{codabux2016technical}. OO-are able to predict maintenance effort more than traditional metrics can\cite{li1993object} by identifying design flaws, and defect-prone, change-prone, and fault-prone classes\cite{basili1996validation}. In addition, OO-metrics may potentially affect the quality attributes of a system. For example, large values of \textit{WMC} will affect a system's maintainability and reusability\cite{quenelobject}. OO-metrics are calculated over data that are extracted from the systems source code. Chidamber and Kemerer's suite of metrics have been applied in empirical investigations of OO-systems by multiple researchers, including Basili et al.\cite{basili1996validation}, Chidamber and Kemerer\cite{chidamber1994metrics}, Okike\cite{okike2010pedagogical}, and Bakar et al.\cite{bakar2014analysis}. We have applied this suite of OO-metrics to discover potential fault-prone classes.

\textit{Understand} C++ is an integrated development environment that enables static code analysis through visualization, documentation, and metric tools. The software is capable of analyzing projects with multiple lines. An academical license tool was provided to us, and the tool was used in our case study to compute software metrics. Every file file in the system were analyzed, and metrics are then extracted from these files. The tool has been used by researchers. Understand have been proven to be useful for code analysis. Malhotra et al.\cite{malhotra2015fault} calculated threshold values of OO-metrics by using statistical models. \textit{Understand} was used to extract relevant metric data from one of the systems. Furthermore, Codabux et al.\cite{codabux2016technical} extracted class-level metrics for defect- and change-prone classes using \textit{Understand}.

Threshold values were derived in this study to assist us on identifying classes with design flaws. Threshold can be defined as the upper bound value value for a metric. A metric value with a greater than its upper bound threshold value can be considered as problematic, while a metric value lower than its upper bound threshold value can be considered as acceptable. Both metric values and threshold values can be compared in design phase of software development to identify the metrics whose value is bigger than its threshold. Results from our study suggests that refactoring can be applied to classes with larger metric value than its threshold. In other words, we were able to predict possible fault-prone classes by applying metric threshold. For instance, a class with a \textit{WMC} value larger than its threshold value indicate high complexity. Fault-prone classes should be used as early quality indicators, and actions should be take based on extent of the problem. For example, the project team may choose to redesign the entire class to achieve the desired metric value. 


% According to one of the lead developers, code standards are used tremendously, and they use a software analysis static tool to find flaws in the code. However, we suspect that OO-metrics are something that is not used by the team which is why we extracted the values in the first place.


\subsubsection{RQ2: What are the effects of DD?} 
The second research question is related to the consequences of having design flaws in safety-critical software. OO-metrics have proven to be indicators of problems in software design. Classes with larger metric values than its threshold values may affect the quality attributes of a system. Our analysis reported multiple classes in which following metric values were larger than its threshold values; \textit{CBO, RFC, WMC, LCOM, CBO, and NOC}. 

Classes with large values of \textit{WMC} are likely to be more application specific, hence affecting the software's understandability, reusability, flexibility, and maintainability quality attributes\cite{rosenberg1998applying,bansiya2002hierarchical}. Moreover, classes with large values of \textit{RFC} may be harder to understand and test, which also affects the software's understandability, testability, maintainability\cite{rosenberg1998applying}. In addition, \textit{RFC} may affect system's functionality and reusability as objects communicates by message passing\cite{bansiya2002hierarchical}. \textit{LCOM} measures the cohesion of a system. Lack of cohesion in a class increases the classe's complexity, which may lead to errors during development. This metric affects the systems efficiency, reusability, and understandability\cite{rosenberg1998applying,bansiya2002hierarchical}. Large values \textit{CBO} complicates a system, since a module is harder to understand, change, reuse, and maintain due to its excessive coupling with other classes. \textit{CBO} evaluates the systems understandability, extendability, efficiency, reusability, testability, and maintainability of a class\cite{rosenberg1998applying,bansiya2002hierarchical}. DIT and NOC metric are related to inheritance which enables reuse. Large values of \textit{DIT} indicates deep hierarchy which constitutes greater design complexity. Deep hierarchy enhances the potential reuse of inherited methods but in trade-off, complexity will increase which affects other quality attributes. \textit{DIT} metric evaluates efficiency, reusability, understandability, and testability\cite{rosenberg1998applying} of a software. \textit{DIT} metric may also be related to flexibility, extendability, effectiveness, and functionality\cite{bansiya2002hierarchical}. Furthermore, \textit{NOC} primarily evaluates efficiency, testability, and resuability of a system\cite{rosenberg1998applying}, but it may also influence flexibility, understandability, extendability, and effectiveness of a system\cite{bansiya2002hierarchical}.

Code smells are manifestations of design flaws that can have negative influence on software quality. Although the results did not investigate the effects of the identified code smells on the system studied, we have reviewed the consequences of code smell that other researchers have discovered. Sjoberg et al.\cite{sjoberg2013quantifying} investigated the relationship between 12 different code smells and maintenance effort. Their result show that none of these code smells were associated with more maintenance effort. Similarly, Hall et al.\cite{hall2014some} state that some smells indicate fault-prone code in some circumstances but that the effect these smells have on faults and software maintainability is small. Lindsay et al.\cite{lindsay2010does} state that not all \textit{Large Class} code smell are bad, some of them could be explained by decisions that perhaps are not entirely under the control of the developer. For example, a choice of particular design pattern may lead to this smell. On the other hand, both Li et al.\cite{li2007empirical} and Dhillon et al.\cite{dhillon2012can} state that bad smells are positively associated with increased error rate in software projects. Furthermore, Olbrich et al.\cite{olbrich2010all} proved that \textit{God Class} code smell have a negative effect on software quality in terms of change frequency, change size, and number of weighted defects. Khomh et al.\cite{khomh2009exploratory} provided evidence that classes with specific code smells are more subject to change than others. 

These articles state that certain types of code smells can have minimal effects, while other types of code smells can have greater effects on the quality attributes of a system. For example, \textit{God Class} code smell will create more maintenance effort than \textit{Dead Code} code smell will. To determine the actual effects of having code smells, the file and class metric in which the code smells are located should be measured and an action should be based on those results.

% Skrive noe om hvordan DD påvirker de ulike kvalitets attributtene til et system. Gjerne gå dypere inn i et komponent for å finne ut av hvilke komponenter som påvirker systemets kvalitetsattributter mest. 

\subsubsection{RQ3: What kind of DD can be found in embedded systems?} 
The third research question is related to our results from the case study. We were able to identify fault-prone classes by applying threshold on the derived OO-metrics, and code smells using automatic static analysis code tools during our case study. We have been able to identify \textit{Duplicated Code}, \textit{Dead Code}, \textit{Speculative Generality}, and \textit{Long Method} as the most common code smells in general. Furthermore, OO-metrics were useful for investigating each individual class and the software in general. For example, we identified possible \textit{Large Class} code smell my examining the metrics. These smells indicate possible fault-prone classes.

%Se etter artikler som har gjort noe tilsvarende. Hva har andre forskere funnet av DD? Hva har vi funnet, er det noen sammenhenger?

\subsubsection{RQ4: How to pay DD?} 
The last research question is related to the management of DD. A common approach to keep DD from growing, or to pay back DD, is to conduct refactoring and re-engineering. Codabux et al.\cite{p8-codabux} mention that refactoring is a common way to manage and ultimately get rid of TD. In addition, refactoring seem improve important internal measures for reusability of OO-classes\cite{moser2006does}. Without refactoring, the design of the program will decay over time. 

Our results show that 5\% of the source code, including the test files, contain duplicated code. Removing duplicated code will reduce the number of lines of code by default. Some duplicated code were found in the same class, while other were spread across multiple classes. Therefore, they must be handled differently. In most cases during our data collection, the same block of code occurred in the same file or class. If the same code of block exists in two different methods of the same class, \textit{Extract Method} refactoring technique should be applied. This technique creates a new method, which can be invoked from both methods that contained duplicated code\cite{fowler1999refactoring}. In addition, we identified some cases where the same block of code occurred in different classes in the system. For example, two identical files were identified in both Component S and B. If that is the case, then \textit{Extract Class} refactoring technique should be applied. This technique creates a new class, superclass, or a subclass, which can be reused by both of the components with duplicated code. However, if the method is a critical part of one class, then the method should be invoked by the other class.

Moreover, we identified 10 \textit{Long Method} code smells. \textit{Extract Method} can be applied to shorten a method. This technique finds parts the method that seem to go nicely together, and creates a new method\cite{fowler1999refactoring}. In some cases, a method may have lots of parameters and eventually temporary variables. If the method has lots of temporary variables to hold the result of an expression, then \textit{Replace Temp with Query} refactoring technique should be applied\cite{fowler1999refactoring}. \textit{Replace Temp with Query} extracts the expression into a method, and all references to the temporary variables will be replaced with the expression. Moreover, this will allow us to reuse the new method in other methods. A method with long parameter list can be slimmed down with \textit{Introduce Parameter Object} and \textit{Preserve Whole Object}\cite{fowler1999refactoring}.

Furthermore, we were able to identify 15 methods with the \textit{Long Parameter List} code smell. Three of the methods were listed as critical. As mentioned in the paragraph above, Long Parameter List can be slimmed down with \textit{Introducted Parameter Object}, and \textit{Preserve Whole Object}. \textit{Introduce Parameter Object} should be used when a group of parameters naturally go together, while \textit{Presever Whole Object} should be applied when a whole object can be sent as parameter in a method call rather than several values from an object. In addition, \textit{Replace Parameter with Method} refactoring technique can be applied when you can get the data in one parameter by making a request of an object you already know about\cite{fowler1999refactoring}.

Our results reveal 1153 hits of \textit{Speculative Generality} code smell, including unused methods, local variables, and static globals. Fowler et al.\cite{fowler1999refactoring} suggest that unused variables and static globals should simply be deleted. Refactoring unused methods can be done by either removing them, or by applying \textit{Inline Method} refactoring technique if a method body is more obvious than the method itself. \textit{Inline Method} will replace calls to the method with the method content and delete the method.

The last code smell we identified is \textit{Dead Code}. This smell includes "commented out" code, unreachable code, and unnecessary "\#includes in header files". The quickest way to conduct refactoring on Dead Code code smell is to delete unused code and unneeded files. Notice that this will reduce the number of lines of code.

To summarize, DD can be paid by conducting necessary refactoring. We believe that refactoring will keep the software quality stable, which ultimately mitigates DD issues. However, it is worth noticing that refactoring not necessary is the solution to pay DD. There may be some cases where refactoring is unlikely to reduce fault-proneness in classes, and may increase fault-proneness in a class instead\cite{hall2014some}. This phenomenon affects some of the OO-metrics. A consideration should be taken where both metrics and other classes affected by the refactoring are involved. For example, refactoring code smells like \textit{Long Method} and \textit{Duplicated Code} may increase the number of methods and coupling. Code smells should be manged by prioritizing the most critical smells. If the goal is to maintain and improve a certain metric value, then remove the smell that allows to improve this metric. Furthermore, we do believe that improving the software metrics and other work practices may be better more beneficial than refactoring code smells to reduce the maintenance effort.



% En tabell som viser sammenheng mellom spørsmål, metode, og bidrag.

% Contribution | Description | Study | Research Question

%A backlog is used to store issues. Even though the backlog does not store TD items, their backlog does include various issues across the system. By making the developers aware of DD items, we do believe that it may be beneficial in long-run.

%Testing is an important part of DD pa
%- Refactoring suggestions.
%- Applying design patterns
%- Re-engineering





\section{Threats To Validity}
\label{sub:threats_to_validity}
Validity is related to how much the results can be trusted\cite{Wohlin:2000:ESE:330775}. We consider threats to the external, internal, and conclusion validity of this study.


\subsection{Internal Validity}
\label{sub:internal_validty}
Interval validity is the degree to which we can conclude that the dependent variable is accounted for by the independent variable\cite{Wohlin:2000:ESE:330775}. In other words, it concerns any factors that could have influenced our study results without the researcher's knowledge. The internal validity threats can be sorted into three categories\cite{Wohlin:2000:ESE:330775}: \textit{single group threats, multiple group threats}, and \textit{social threats}. \textit{Single group threats} apply to experiments with single groups, which is relevant in our case. One factor is \textit{instrumentation}, that is, the effect caused by the artifacts used for experiment execution\cite{Wohlin:2000:ESE:330775}. We have used multiple tools to analyze the system. In some cases, these tools may result false positives, which may affect our results.

Another factor that may reduce the internal validity is the selection of subjects from a larger group\cite{Wohlin:2000:ESE:330775}. We found our subject for the case study after being recommended by others. The internal validity would have been stronger if the subjects had been selected using random sampling.


%For metrikker, skriv noe om at det vi har regnet er basert på systemet. Om ting hadde vært annerledes ville metrikkene vært annerledes.

\subsection{External Validity}
\label{sub:external_validity}
External validity refers to the degree to which the results from our study can be generalized to industrial practice\cite{Wohlin:2000:ESE:330775}. This study has used one commercial system within a single application domain, so the threat to external validity increases. That said, we cannot generalize the results to other safety-critical systems, both open-source and commercial. More studies need to be conducted on different systems, both commercial and open-source, to compare with the results obtained in this thesis.


\subsection{Conclusion Validity} % (fold)
\label{sub:conclusion_validity}
Conclusion validity refers to the degree in which correct conclusion can be drawn from the relationship between treatment and the outcome\cite{Wohlin:2000:ESE:330775}. One important issue here was the sample size of the experiment. This study has only studied one commercial system, so the statistical power is very low. This is something we are aware of, and therefore more deeper studies need to be performed to confirm if our results have a more general applicability. Another threat may potentially be \textit{"Fishing and error rate"}. Searching or \textit{"fishing"} for a specific result is a threat\cite{Wohlin:2000:ESE:330775}, since the researchers may influence the result by looking for a specific outcome. We do not think that we have influenced the results. Reliability of measures may be another threat. Running the tools twice gave us the same results. However, it is not guaranteed that the outcome would be same if another tool had been applied. Another potential problem can be the lack of experience on conducing case studies.


% subsection subsection_name (end)

% subsection construct_validity (end)







% Prøve å få til evaluering av verktøyene og metrikkene. Svar på forskningspørsmå, hold det temarettet.

% It is worth noticing that not all results can be considered as a consequence of good design. At least some of the results can be explained by decisions that are perhaps not entirely under the control of the developer. For instance, time pressure is revealed as a common cause for TD accumulation. A company may compress a schedule to a point where engineers need to compromise design-time qualities to run-time qualities.

% The results may also explained due to miscommunication within the team. Communication within a project team plays a big part in achieving higher software quality. A common way to handle DD issues is to make sure that the developers are aware of the OO-metrics and its corresponding threshold values. For example, if the developers are aware of the classes in which have larger metric values than its threshold values, the debt would in long-term be less significant. 

% Software companies should constantly include metrics in design and development phase. Metrics about complexity and OO-design will give an insight in system size and quality attributes, such as the possiblity for reuse, maintenance, and testability. OO metrics can prompt potential code problems and refactoring need will be minimized. 

\cleardoublepage