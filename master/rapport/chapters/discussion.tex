% !TEX encoding = UTF-8 Unicode
% !TEX root = ..\main.tex
% !TEX spellcheck = en-US

\chapter{Discussion}
This thesis aims to investigate the possibilities to identify design debt in embedded systems. Our case study evaluated a software system written in C/C++. This chapter will discuss the results we gathered during our case study. Section 5.1 contains a discussion the measured values for object-oriented metrics by applying a set of threshold values to uncover possible unsecure parts of the code. Section 5.2 will lo



\section{Evaluation of Object-Oriented Metrics by Applying Threshold Values}
Measuring software metrics in object-oriented software is important in terms of quality management\cite{tarcisio,ferreira2012identifying}, as software metrics can be used as predictors of fault-prone classes in object-oriented systems\cite{basili1996validation}. A study by Basili et. al\cite{basili1996validation} assessed Chidamber and Kemeerers\cite{chidamber1994metrics} suite of object-oriented metrics as predicators of fault-prone classes. Their results implied that object-oriented metrics appear to be useful to predict class fault-proneness during early phases in software life-cycle. Object-oriented measurements alone are not necessary sufficient to identify parts of the system with major design violations. According to Tarcisio, software metrics are not effectively used in software industry due to the fact that for the majority of metrics, thresholds are not defined\cite{tarcisio}. Threshold is defined as values used to set ranges of desirable and undesirable metric values for measured software\cite{ferreira2012identifying}. Knowing thresholds for metrics allow us to assess the quality of a software, and we may be able to identify where in a design errors are likely to occur. Lanza et al.\cite{lanza2007object} presents two ways to identify major sources for threshold values; statistical information (i.e., thresholds that are based on statistical measurements), and general accepted semantics (i.e., thresholds that are based on information which is considered common).  

Threshold values were derived for the individual metrics using their descriptive statistics from Table \ref{tab:oometrics-firmus} in order to identify the classes with major design violations. Threshold values have been identified using statistical information\cite{lanza2007object}. For each metric, we used the the sample mean and standard deviation from Table \ref{tab:oometrics-firmus} to derive its corresponding threshold value. The first threshold value is corresponds with the mean and represents the most typical value in the data set\cite{cais2014identifying}. According to Lanza et al.\cite{lanza2007object}, the first threshold value can be calculated by subtracting the standard deviation from the sample mean. However, the standard deviation in our data set may be larger than the sample mean, which leads to negative threshold values. The second threshold value the sum of the sample mean and the standard deviation. It represents high, but still acceptable values. The third threshold value is simply the second threshold value multiplied with 1.5\cite{lanza2007object}. It is considered as extreme, and should not be included in the data set. In addition, we have gathered thresholds that has been proposed by researchers for the metrics we have measured in this thesis. Table \ref{tab:thresholds} presents the metrics and their threshold values.

\begin{table}[ht!]
\resizebox{\textwidth}{!}{
\centering
\caption{Thresholds for object-oriented software metrics}
\label{tab:thresholds}
\begin{tabular}{|l|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{3cm}|}
\hline
\textbf{Metric} & \textbf{Observed Value} 	& \textbf{Low} 		& \textbf{High}  & \textbf{Extreme value} 	& \textbf{Recommended Max Value} \\ \hline
LCOM   			& 100			         	& 42				& 75   			 & 100+						& 72.5\cite{tarcisio}           \\ \hline
DIT 			& 4             			& 1					& 2				 & 3+						& 4\cite{tarcisio}, 5\cite{rosenberg1999risk,metricoverview,metricsguide}              \\ \hline
CBO    			& 30  			          	& 6					& 11			 & 17+						& 5\cite{rosenberg1999risk,metricsguide}, 14\cite{sahraoui2000can,phpdepend}     \\ \hline
NOC    			& 20 			           	& 0					& 2				 & 4+						& 3\cite{tarcisio}, 5\cite{metricsguide}, 10\cite{metricoverview}               \\ \hline
RFC    			& 115           			& 15				& 34			 & 51+						& 50\cite{rosenberg1999risk}      \\ \hline
NIM    			& 48            			& 8					& 15			 & 23+						&               \\ \hline
NIV    			& 18            			& 2					& 5				 & 7+						&             \\ \hline
NOM    			& 48            			& 9					& 16			 & 24+						& 20\cite{rosenberg1999risk}                \\ \hline
WMC    			& 325  			         	& 19				& 51			 & 76+						& 34\cite{tarcisio}, 40\cite{rosenberg1999risk}, 50\cite{phpdepend}      \\ \hline
\end{tabular}}
\end{table}


For 101-1000 classes:

CBO: Good: 0-1, regular: 2-20, bad: 20+
NIV: 0-1, 1-8, 8+
NIM: 0-25, 6-50, 50+
DIT: 2
LCOM: 0, 1-20, 20+


ARTIKLER: 

WMC, DIT, RFC CBO, and NOC metrics appear to be useful to predict fault-prone classes. 

WMC: It was shown to be somewhat significant, and their results were stronger for new and modified classes. As excpected, the larger WMC is, the larger the probability of fault detection. Internal complexity does not have a strong impact if the class is reused verbatim or with very slight modifications. In that case, class interface properties will have the most significant impact.

DIT: Shown to be very significant. The larget DIT, the larger probability of defect detection.

RFC: Very significant. The larger RFC, the larger probability of defect detection, especially for UI classes and new/modified classes, same reason as for WMC for extensively modified classes.

NOC: Very significant except in the case of UI casses, but the trend seem to be a contrary to what they expected The larger the NOC value, the lower the probability of defect detection. This trend can be explained by that most classes do not have more than one child, and that verbatim reused classes are somwwhat associated with large NOC.

CBO: Very significant, and particular for UI classes.

LCOM: NIV kan si noe om LCOM. Si at det er 10 instanse variabler, det er ikke sikkert at alle blir delt på mellom metodene i klassen. LCOM is also significcant for predicting fault proneness.

These metrics can be used to build a predictive model of fault-prone classes.

Their result show these values:
WMC: Max 99, min 1, med 9.5, mean 13.4, stdev 14.9
DIT 9,0,0,1.32,1.99
RFC 105, 0, 19.5, 33.91, 33.37
NOC 13 0 0 0.23, 1.54
LCOM 426, 0, 0, 9.7, 63.77
CBO 30 0 5 6.8 7.56


% Hva betyr våre findings, hvor verdifulle de kan være, og hvorfor.

\subsubsection{LCOM}
LCOM is related to the counting of methods using common attributes in a class. In our experiment, we observed that LCOM median value for Project Firmus is 55, which is very similar to LCOM media value measured by Chidamber and Kemerer\cite{chidamber1994metrics}, Okike\cite{okike2010pedagogical}, and Basili et. al\cite{basili1996validation}. A low median value indicates that at least 50\% of the classes are cohesive.

By following Chidamber and Kemerer's guide of interpreting th



 We applied the extreme threshold value to the system studied, and identified two classes with low cohesion. However, by analyzing the frequency distribution of LCOM values in Figure "X", we clearly see that the normal distrubution is not normal. That may be the reason we got our values in the first place, and why it would be better to use one of the recommended max values as threshold. The recommended threshold value for LCOM is 72.5, which is very close to our "high" value. By applying the recoomended threshold, we identified 41 classes with LCOM value larger than 72.5. 

We observe that lack of cohesion values seem increasing with the size of classes which is plausible. In reality, large classes tend to lack cohesion. These classes tend to have a relatively high number of attributes and methods. Moreover, high average values of LCOM can be caused by a large number of attributes and methods in the class, where many of the methods does not use the same attributes. By observering our results, we noticed that classes with zero instance variables tend to have high cohesion.

Viewpoints: 
- Cohesiveness of methods within a class is desirable, since it promotes encapsulation
- Lack of cohesion implies classes should probably be split into two or more subclasses
- Any measure of disparateness of methods helps identify design flaws in classes
- Low cohesion increases complexity, thereby increasing the likelihood of errors during the development process.




\subsubsection{Depth in Inheritance Tree}
%DIT Viewpoints from Chidamber:
%%- The deeper a class is in the hierarchy, the greater number of methods it is likely to inherit, making it more complext to predict its behaviour. 
%- Deeper trees consistute greater design complexity, since more methods and classes are involved.
%- The deeper a class is in the hierachy, the greater the potential reuse of inherited methods.
% In general, as the tree gets deeper, it constitutes greater design complexity as more classes and methods are involved. 
DIT indicates how deep a class is in the inheritance tree. It is evident that a deep inheritance makes software maintenance more difficult("SITER DALY et al. 1996"). Higher degree of DIT indicate a trade-off between increased complexity and increase reuseability. By following Chidamber and Kemerer's\cite{chidamber1994metrics} guide to interpreting their DIT metric using descriptive statistics, a low median value indicates that at least 50\% of the classes tend to be close to the root in the inheritance hierarchy. In their study, a low median value had a typical value of 1 and 3. However, if there is a majority of DIT values below 2, it may represent poor exploitation of the advantages of object-oriented design and inheritance, because a DIT value of 2 and 3 indicates higher degree of reuse. In Table \ref{tab:oometrics-firmus}, we observe that DIT median value for Project "Firmus" is 1. More precisely, approximately 39\% of the classes have a DIT value of 0, while 26.7\% of the classes have a DIT value of 1. The classes are considered to be close to the root in the inheritance tree, and there may be a probability of not exploiting the advantages of object-oriented metrics.

Classes with high values of DIT have shown to be very significant in identifying fault-prone classes\cite{basili1996validation}. We derived the following threshold for the DIT metric to identify classes with high values of DIT: a low/good value of 1, a high/typical value of 2, and extreme/bad value of 3. By applying these thresholds, we observe that at least 26.7\% of the classes satisfies good value. However, the extreme/bad value of 3 does not comply with the median value of 3 which Chidamber and Kemerer\cite{chidamber1994metrics} considers as a good value. Therefore, we have chosen to apply the recommended max value of 5 seems as it has been recommended by other researchers. We were not able to identify any classes with a DIT value of 5 or more, but we identified two classes with DIT value of 4. These results indicates little use of inheritance and hence, polymorphism. 

The lower the class is in the inheritance three; the larger is its DIT, and therefore, the harder is the class maintainability. 

\subsubsection{Number Of Children}
%NOC Viewpoints:
%- Greater the number of children, grater the reuse, since inheritance is a form of reuse
%- Greater the number of children, greater the likelihood of improper abstraction of the parent class. If a class have a large number of children, it may be a case of misuse of subclassing.
%- The number of children gives an idea of the potential influece a class has on the design. If a class has a large number of children, it may require more testing of the methods in that class
NOC measures the number of children a class has. In general, the greater number of children, the greater potential of reuse, since inheritance is a form of reuse. However, if a class have a large number of children, it may be a case of misuse of subclassing\cite{basili1996validation}. Higher degrees of NOC indicates increase in reuse, but in trade-off, the classes may require more testing. In Table \ref{tab:oometrics-firmus}, we observe that NOC median value is 0. The distribution of NOC metric shows that approximately 86.5\% of all classes have no children, and that a small number of classes have many immediate subclasses. Both Chidamber and Kemerer\cite{chidamber1994metrics}, and Basili et al.\cite{basili1996validation} have observed the similar median values for NOC metric in their respective studies. These values suggests that designer may not be fully exploiting the advantages of inheritance as a basis for designing classes. Lack of communication could be another reason between class designers, which leads developers not to reuse.

Following threshold value for NOC metric were derived from its descriptive statistics in Table \ref{tab:oometrics-firmus}: a low/good value of 0, a high/typical value of 2, and extreme/bad value 4. These values are comparable to the thresholds that Filo et al.\cite{tarcisio} have identified in their study. The difference is that they classify extreme/bad value as 4. By applying these thresholds, we observe that at 91.2\% of the classes are classified as low/good, 5.2\% of all classes are classified as high/typical, and 3.6\% of all classes are classified as extreme/bad. These classes are an indication of something that may be hard to understand an maintain. These classes should be reviewed.



\subsubsection{Coupling Between Object classes}

%Viewpoints:
%- Excessive coupling between object classes is detrimental to modular design and prevents reuse. The more independent a class is, the easier it is to reuse it in another application.
%- In order to improve the modularity and promote encapsulation, inter-oject class couples should be kept to a minimum. The larger number of couples, the higher the sensitivity to changes in other parts of the design, and therefore maintenance is more difficult.
%- A measure of couping is useful to determine how complex the testing of varioous parts of a design are likely to be. The higer the inter-object class coupling, the more rigorous the testing needs to be.
CBO refers to the number couplings between object classes. Higher values of CBO indicates the extent of lack of reuse potential of a class, and that more effort is required to maintain and test the class. According to Chidamber and Kemererer\cite{chidamber1994metrics}, a low median had a typical value of 0, while a high median had a value of 9. A median value of 0 suggests that at least half of the classes are self-contained and do not refer to other classes. Basili et al.\cite{basili1996validation} states that CBO appears to be useful to predict class fault-proneness. Our results revealed a CBO median value of 5, indicating that at least 50\% of the classes refers to 5 or less object classes. The CBO value is generally less for most classes, hence most of these classes are easy to understand, reuse, and maintain. We did notice that some of these classes had higher values in the other metrics, such as WMC and LCOM. 

Threshold values for the CBO metric were derived, and the following values were identified:  a low/good value of 6, a high/typical value of 11, and an extreme/bad value of 17 or more. Comparing our thresholds, we do think that the extreme/bad value may be a little bit high, especially when we compare the values with the recommended max values. Moreover, C\&K did classify a median value of 9 as high. Therefore, we chose to use high/typical value as our default threshold. Bay applying the threshold, we observe that 11.3\% of all classes have a coupling of 12 or more. To promote encapsulation and improve modularity of a class, coupling between classes should be kept to a minimum. As the identified classes have high coupling values, they may be more sensitive to changes in other parts of the design, leading to maintenance problems. Moreover, it is likely that these classes are difficult to reuse in other parts of the system, and more complex to test.

CBO can be used as a way to track whether the class hierarchy is losing its integrity, and whether different parts of the system are developing unnecessary relations between classes.



\subsubsection{Response For Class}
%Viewpoints:
%- If a large number of methods can be invoked in response to a message, the testing and debugging of the class becomes more complicated since it requires a greater level of understanding required on the part of the tester. 
%- The larger the number of methods that can be invoked from a class, the grater the complexity of the class.
%- A worst case of value for possible responses will assist in appropriate alocation of testing time.
RFC is defined as the total number of methods that can be executed in response to a message to a class. This count includes all methods available in the class hierarchy. 



Study results:
- Site A: Median: 6, Max: 120, Min: 0
- Site B: Median: 28, Max: 422, Min: 3

\subsubsection{Number Of Methods and Number of Instance Methods}
Generelt ønsker man å ha flere små metoder i en klasse enn et par store. Dersom LCOM ikke stemmer kan klassen bli veldig stor og det kan også si ne om at klassen er en code smell som bør splittes opp.

\subsubsection{Number of Instance Variables}



\subsubsection{Weighted Methods per Class}
There may be many methods in a class, hence WMC2 not giving good results. If a class has many methods, some methods may have low cmoplexity while others have high complexity.

OUr threshold values: Low/good: 19				High/typical: 51	Extreme/bad: 76. By comparing these values with the recommended max value, we do notice that practicioners recommends a max value close to value we regard as high/typical. Therefore, it seems like this value is more appropriate to use to identify the classes with design flaws. By applying the threshold, we identified 16 classes with a WMC higher than 51. These classes can be considered as problematic.


The classes above the recommended values can be seen as outliners. 

According to Chidamber: 
- Number and complexity of methods may be an indicator of how much time and effort is required to develop and maintain the class.
- Larger number of methods in a class, the greater the potential impact on chilcren since they will inherit all the methods in the defined class.
- A class with many methods are likely to be more application specific, hence limiting the possibility of reuse.

Their study results on WMC
- Site A: Median: 5, Max: 106, Min: 0
- Site B: Median: 10, Max: 346, Min: 0




Classes which are infected with God Class, and God Methods have higher class error probability than non-infected classes.




\section{Research Evaluation}
%Table "X" contains a summary of the resuls of this research. Although, the research did not specify the effects of design debt in embedded systems, we still were able to identify some of the effects it had on

%The goal with this case study conducted at Autronica in Trondheim is to find ways that can help us to identify design debt in embedded systems. One approach of doing this is to measure the software quality using object-oriented metrics. Using object-oriented metrics to measure the system can help us to identify poorly designed classes, which also helps us to answer the first research question.

This thesis is focused on identifying design debt in embedded systems. In particular, we were able to identify design debt by measuring object-oriented metrics and detecting code smells. The software design was analyzed using a suite of object-oriented metrics proposed by Chidamber and Kemererer, one of the most used object-oriented measures. According to our results, we can now answer the research questions that were stated in Chapter 1.





WMC, DIT, RFC CBO, and NOC metrics appear to be useful to predict fault-prone classes. 

WMC: It was shown to be somewhat significant, and their results were stronger for new and modified classes. As excpected, the larger WMC is, the larger the probability of fault detection. Internal complexity does not have a strong impact if the class is reused verbatim or with very slight modifications. In that case, class interface properties will have the most significant impact.

DIT: Shown to be very significant. The larget DIT, the larger probability of defect detection.

RFC: Very significant. The larger RFC, the larger probability of defect detection, especially for UI classes and new/modified classes, same reason as for WMC for extensively modified classes.

NOC: Very significant except in the case of UI casses, but the trend seem to be a contrary to what they expected The larger the NOC value, the lower the probability of defect detection. This trend can be explained by that most classes do not have more than one child, and that verbatim reused classes are somwwhat associated with large NOC.

CBO: Very significant, and particular for UI classes.

LCOM: NIV kan si noe om LCOM. Si at det er 10 instanse variabler, det er ikke sikkert at alle blir delt på mellom metodene i klassen. LCOM is also significcant for predicting fault proneness.

These metrics can be used to build a predictive model of fault-prone classes.




\subsubsection{RQ1: How can design debt be identified?} 
The first research questions is related to the techniques we have used to identify design debt in this research. Both automatic static code analysis, and by measuring object-oriented metrics using Chidamber and Kemerer's suite of metrics have proven to be useful in the context of design debt identification. We were able to collect a large amount of measurements that characterize the software by using various tools. Tools that have been used thorough this research are Doxygen, Understand, SonarQube, SourceMonitor, CCCC, CppCheck, CppDepend, and Enterprise Architect.

Code smells are an example of design flaws that can degrade maintainability of source code, which implies that code smells can be used as an indicator to identify fault-prone files in the system. Moreover, code smells are an indication of refactoring possibilities in code base. We have been able to identify Duplicated Code, Long Method, Long Parameter List, Speculative Generality, Dead Code, and Large Classes code smells in our analysis. Duplicated Code were identified using SonarQube. SonarQube analyzed the entire source code base, and identified similar code blocks that had an appearance in multiple places. Long Method, Speculative Generality, and Dead Code were identified using CppDepend, Understand and CCCC. CppDependLong Parameter List code smell were detected using CppDepend.

Another approach to assessing the software quality is based on object-oriented metrics\cite{codabux2016technical}. Object-oriented metrics can be used to identify design flaws, and defect-prone, change-prone, and fault-prone classes. In addition, object-oriented metrics affect the quality attributes of a system. For example, large values of WMC will affect a systems maintainability and reusability\cite{quenelobject}. Object-oriented metrics are calculated over data that are extracted from the systems source code. The most well-known suite of software metrics for object-oriented systems is desribed by Chidamber and Kemerer. This suite of metrics have been applied in empirical investigations of object-oriented systems by multiple researchers, including Basili et al.\cite{basili1996validation}, Chidamber and Kemerer\cite{chidamber1994metrics}, Okike\cite{okike2010pedagogical}, and Bakar et al.\cite{bakar2014analysis}. Moreover, we have applied this suite of object-oriented metrics to discover potential fault-prone classes.

Understand is an integrated development environment that enables static code analysis through visuals, documentation, and metric tools. The software is capable of analyzing projects with multiple lines. An academical license tool was provided, and the tool was used in our case study to compute software metrics. Each file in the system is analyzed, and metrics are then extracted from these files. The tool has been used by researchers. Understand have been proven to be useful for code analysis. Malhotra et al.\cite{malhotra2015fault} calculated threshold values of object-oriented metrics by using statistical models. Understand was used to extract relevant metric data from one of the systems. Codabux et al.\cite{codabux2016technical} extracted class-level metrics for defect- and change-prone classes using Understand.

By applying thresholds for object-oriented metrics, we were able to identify the classes with potential design flaws in which inspection is needed. Threshold can be defined as the upper bound value value for a metric. A metric value with a greater than its upper bound threshold value can be considered as problematic, while a metric value lower than its upper bound threshold value can be considered as acceptable. Both metric values and threshold values can be compared in design phase of software development to identify the metrics whose value is bigger than its threshold. Based on the results, an alternative design structure can be applied, and refactoring can be applied to classes with larger metric value than its threshold. In other words, threshold values can be used to predict possible fault-prone classes that need to be inspected. For instance, a class with its WMC value larger than its threshold value will indicate high complexity for that particular class. Such classes should be used as early quality indicators, and actions should be take based on extent of the problem. For example, the project team may choose to redesign the entire class in order to achieve the desired metric value. Basili et al.\cite{basili1996validation} states that WMC, DIT, NOC, CBO, and RFC are useful metrics to predict fault-prone classes. 

%Although we applied modern techniques to threshold identification, our study still should be viewed as evaluation of one software system and our results should not be taken as a dogma. Our techniques can be  applied to another kind of software for which it is hard to gather data, but it is necessary to bear in mind, that the thresholds represent only local data and for general usage, the broader comparison should e used. We present our data as a base for possible future comparison with other studies and we hope our results can bring more light into the thresholds of safety critical software metrics. As a future endeavor,  e would like to study differences between metrics thresholds measured in this study and data from a system with at least some degree of similarity. We would like to compare our results with measurement of  hresholds of open-source operational systems (which appear to be the most similar to our software) 



% According to one of the lead developers, code standards are used tremendously, and they use a software analysis static tool to find flaws in the code. However, we suspect that object-oriented metrics are something that is not used by the team which is why we extracted the values in the first place.


\subsubsection{RQ2: What are the effects of design debt?} 
The second research question is related to the effects of having design flaws in embedded software.

Classes with larger metric values than its threshold values may affect the quality attributes of a system. Our analysis reported multiple classes in which following metric values were larger than its threshold values; CBO, RFC, WMC, LCOM, CBO, and NOC. Classes with large values of WMC are likely to be more application specific, hence affecting the software's understandability, reusability, flexibility, and maintainability quality attributes\cite{rosenberg1998applying,bansiya2002hierarchical}. Moreover, classes with large values of RFC may be harder to understand and test, hence affecting the software's understandability, testability, maintainability\cite{rosenberg1998applying}. In addition, RFC may affect system's functionality and reusability as objects communicates by message passing\cite{bansiya2002hierarchical}. LCOM measures the cohesion of a system. Lack of cohesion in a class increases is complexity, which ultimately leads to errors during development. This metric affects the systems efficiency, reusability, and understandability\cite{rosenberg1998applying,bansiya2002hierarchical}. Large values CBO complicates a system, since a module is harder to understand, change, reuse, and maintain due to its excessive coupling with other classes. CBO evaluates the systems understandability, extendability, efficiency, reusability, testability, and maintainability of a class\cite{rosenberg1998applying,bansiya2002hierarchical}. Furthermore, the DIT and NOC metric are related to inheritance which enables reuse. Large values of DIT indicates deep hierarchy which constitute greater design complexity. Deep hierarchy enhances the potential reuse of inherited methods but in trade-off, complexity will increase which affects other quality attributes. In total, this metric evaluates efficiency, reusability, understandability, and testability\cite{rosenberg1998applying}. In addition, DIT metric may also be related to flexibility, extendability, effectiveness, and functionality\cite{bansiya2002hierarchical}. NOC primarily evaluates efficiency, testability, and resuability of a system\cite{rosenberg1998applying}, but it may also influence flexibility, understandability, extendability, and effectiveness of a system\cite{bansiya2002hierarchical}.

We have not been able to study the effects of code smells discovered in this research. However, we have analyzed effects of code smell that other researchers has discovered. Sjoberg et al.\cite{sjoberg2013quantifying} investigated the relationship between 12 different code smells and maintenance effort. Their result revealed that none of these code smells were associated with more maintenance effort. Similarly, Hall et al.\cite{hall2014some} state that some smells indicate fault-prone code in some circumstances but that the effect these smells have on faults is small. In some cases, spending effort on refactoring smells may have no effect on the system. On the other hand, both Li et al. and Dhillon et al. state that bad smells are positively associated with increased error rate in software projects. Furthermore, Olbrich et al. proved that God Class and Brain Class code smell have a negative effect on software quality in terms of change frequency, change size, and number of weighted defects. However, as the smells were normalized with respect to size, the results were opposite. 

Artiklene viser at code smells som regel har minimale virkninger. Noen code smells kan ha andre virkninger på systemet, og påvirke systems kvalitets attributter. 

We conclude with that the effects of having code smells may be low, and that its file or class metric should be measured.


% Skrive noe om hvordan design debt påvirker de ulike kvalitets attributtene til et system. Gjerne gå dypere inn i et komponent for å finne ut av hvilke komponenter som påvirker systemets kvalitetsattributter mest. 

\subsubsection{RQ3: What kind of design debt can be found in embedded systems?} 

The third research question is related to our results from the case study. We have been able to identify code smells in the analyzed system, and possible fault-prone classes by applying threshold values on the derived metrics.

We have found code smells in form of dead code, duplicated code, speculative generality, 

%Se etter artikler som har gjort noe tilsvarende. Hva har andre forskere funnet av design debt? Hva har vi funnet, er det noen sammenhenger?

\subsubsection{RQ4: How to pay design debt?} 
- Refactoring suggestions.
- Applying design patterns

The following subsections discuss how refactoring can be applied to the identified code smells.

Our results revealed that 5\% of the source code, including the test files, contains duplicated code. 

Removing duplicated code will reduce the number of lines of code. However, duplicated code in different locations must be handled differently.

We identified same code in two or more methods. By applying the "Extract Mehotd" refactoring technique, a new method is created. The methods which contains the duplicated code will call on this method.

If duplicated code are found in two or more different classes, the "Extract Superclass" refactoring technique can be applied. This allows us to create a single superclass for these classes that maintains all their prevous functionality. In this case, number of lines of code will increase, and so will the NOC metric. On the other hand, if there are duplicated in

For example, we identified two identical file in both Component B and Component S. By 

How to refactor, does the refactoring affect some of the metrics

How to refactor, does the refactoring affect some of the metrics

How to refactor, does the refactoring affect some of the metrics

How to refactor, does the refactoring affect some of the metrics
The quickest way to refactor dead code is to delete unused code and unneeded files. Removing dead code will reduce the number of lines of code. 

How to refactor, does the refactoring affect some of the metrics
-

A common approach to keep design debt from growing, or to pay back design debt, is to conduct refactoring and re-engineering. Codabux et al.\cite{p8-codabux} does mention that refactoring is a common way to manage technical debt. 

We believe that by applying refactoring will keep the software quality stable, which ultimately mitigates design debt issues.




They conclude with that refactoring is unlikely to reduce fault-proneness and in some cases, they may increase fault-proneness.(hall)

Refactoring is the process of improving a software design without changing its external behaviour\cite{fowler1999refactoring}. 



\section{Threats To Validity}
\label{sub:threats_to_validity}
Validity is related to how much the results can be trusted\cite{Wohlin:2000:ESE:330775}. We consider threats to the external, internal, and construct validity of this study.

\subsection{Internal Validity}
\label{sub:internal_validty}
Interval validity is the degree to which we can condulcde that the dependent variable is accounted for by the independen varuable.



For metrikker, skriv noe om at det vi har regnet er basert på systemet. Om ting hadde vært annerledes ville metrikkene vært annerledes.

\subsection{External Validity}
\label{sub:external_validity}
External validity refers to the degree to which the results from the study can be generalized to the population. The system investigated in this study consists of one simple size and an application domain, hence increasing the threat to external validty. 

\subsection{Construct Validity} % (fold)
\label{sub:construct_validity}

\subsection{Conclusion Validity} % (fold)
\label{sub:conclusion_validity}
Conclusion validity refers to the degree in which correct conclusion can be drawn from the relationship between treatment and the outcome. Our case study consisted of studying one system, so in general, the statistical power is very low. Deeper studies needs to be performed to confirm is our results have more applicability.

% subsection subsection_name (end)

% subsection construct_validity (end)







% Prøve å få til evaluering av verktøyene og metrikkene. Svar på forskningspørsmå, hold det temarettet.

It is worth noticing that not all results can be considered as a consequence of good design. At least some of the results can be explained by decisions that are perhaps not entirely under the control of the developer. For instance, time pressure is revealed as a common cause for technical debt accumulation. A company may compress a schedule to a point where engineers need to compromise design-time qualities to run-time qualities.

The results may also explained due to miscommunication within the team. Communication within a project team plays a big part in achieving higher software quality. A common way to handle design debt issues is to make sure that the developers are aware of the object-oriented metrics and its corresponding threshold values. For example, if the developers are aware of the classes in which have larger metric values than its threshold values, the debt would in long-term be less significant. 

We do believe that if developers thinks about metrics, they will ultimatly 